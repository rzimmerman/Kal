grammar  = require './grammar'
KEYWORDS = grammar.KEYWORDS

NOPAREN_WORDS = ['is','otherwise','except','else','doesnt','exist','exists','isnt','inherits',
                 'from','and','or','xor','in','when','instanceof','of','nor','if','unless',
                 'except','for','with','wait','task','fail','parallel','series','safe','but',
                 'bitwise','mod']

function translate_sugar (tokens, options, tokenizer)
  out_tokens = coffee_style_functions print_statement noparen_function_calls multiline_statements multiline_lists clean code_in_strings tokens, tokenizer
  if options?.show_tokens
    debug = []
    for t in out_tokens
      if t.type is 'NEWLINE'
        debug.push '\n'
      else
        debug.push t.value or t.type
    console.log debug.join ' '
  return out_tokens
exports.translate_sugar = translate_sugar

function clean (tokens)
  # close out with a newline in case the user did not, remove whitespace
  out_tokens = []
  for token in tokens
    if token.type isnt 'WHITESPACE'
      out_tokens.push(token)
    else if out_tokens.length > 0
      out_tokens[out_tokens.length - 1].trailed_by_white = yes
  return out_tokens

function multiline_statements (tokens)
  # allow multi-line statements with line breaks after commas
  out_tokens = []
  last_token = null
  continue_line = no
  reduce_dedent = 0

  for token in tokens
    skip_token = no
    if last_token?.value in [','] and token.type is 'NEWLINE'
      continue_line = yes
      skip_token = yes
    else if continue_line
      if token.type is 'INDENT'
        skip_token = yes
        reduce_dedent += 1
      else if token.type is 'NEWLINE'
        skip_token = yes
      else if token.type is 'DEDENT'
        if reduce_dedent > 0
          reduce_dedent -= 1
          skip_token = yes
          if reduce_dedent is 0
            out_tokens.push {text:'\n', line:token.line, value:'',type:'NEWLINE'}
        else
          out_tokens.push last_token #add back in the newline
    out_tokens.push(token) unless skip_token
    last_token = token
  return out_tokens

function noparen_function_calls (tokens)
  # allow function calls without parentheses
  out_tokens = []
  close_paren_count = 0
  last_token = null
  triggers = []
  closures = []
  ignore_next_indent = no

  i = 0
  while i < tokens.length
    token = tokens[i]
    last_token_isnt_reserved = not (last_token?.value in KEYWORDS) or tokens[i-2]?.value is '.'
    last_token_callable = (last_token?.type is 'IDENTIFIER' and last_token_isnt_reserved) or last_token?.value is ']'
    token_isnt_reserved = not (token.value in NOPAREN_WORDS)
    this_token_not_operator = ((token.type in ['IDENTIFIER','NUMBER','STRING','REGEX'] or token.value is '{' or (token.value is '[' and last_token?.trailed_by_white)) and token_isnt_reserved)
    paren_from_expanded_string = (token.value is '(' and token.soft)
    if last_token_callable and (this_token_not_operator or paren_from_expanded_string)
      triggers.push 'NEWLINE'
      out_tokens.push {text:'(', line:token.line, value:'(', type:'LITERAL'}
      closures.push ')'
    else if (token.value is 'function' or (token.value is '>' and last_token?.value is '-')) and triggers[triggers.length-1] is 'NEWLINE'
      triggers[triggers.length-1] = 'DEDENT'
      ignore_next_indent = yes
    else if token.type is 'INDENT'
      if ignore_next_indent
        ignore_next_indent = no
      else
        triggers.push 'DEDENT'
        closures.push ''
    else if token.type is 'NEWLINE' and tokens[i+1]?.type isnt 'INDENT'
      ignore_next_indent = no

    if (token.type is 'NEWLINE' or token.value in ['if','unless','when','except']) and closures.length > 0 and triggers[triggers.length - 1] is 'NEWLINE'
      while closures.length > 0 and triggers[triggers.length - 1] is 'NEWLINE'
        triggers.pop()
        closure = closures.pop()
        out_tokens.push({text:closure, line:token.line, value:closure, type:'LITERAL'}) if closure isnt ''
      out_tokens.push token
    else if token.type is 'DEDENT' and closures.length > 0 and triggers[triggers.length - 1] is 'DEDENT'
      out_tokens.push token
      triggers.pop()
      closure = closures.pop()
      out_tokens.push({text:closure, line:token.line, value:closure, type:'LITERAL'}) if closure isnt ''
    else if closures.length is 0 or token.type isnt triggers[triggers.length - 1]
      out_tokens.push token
    last_token = token
    i += 1
  while closures.length > 0
    closure = closures.pop()
    out_tokens.push({text:closure, line:token.line, value:closure, type:'LITERAL'}) if closure isnt ''
  return out_tokens

function coffee_style_functions (tokens)
  #allow function definitions with the -> operator
  out_tokens = []
  last_token = null

  i = 0
  while i < tokens.length
    token = tokens[i]
    if last_token?.value is '-' and token?.value is '>'
      out_tokens.pop() # remove the dash
      new_tokens = []
      t = out_tokens.pop()
      if t?.value is ')'
        while t?.value isnt '('
          new_tokens.unshift t
          t = out_tokens.pop()
        new_tokens.unshift t
      else
        out_tokens.push t
        new_tokens.push {text:'(', line:token.line, value:'(', type:'LITERAL'}
        new_tokens.push {text:')', line:token.line, value:')', type:'LITERAL'}
      f_token = {text:'function', line:token.line, value:'function', type:'IDENTIFIER'}
      new_tokens.unshift f_token
      out_tokens = out_tokens.concat new_tokens
    else
      # push the current token unchanged
      out_tokens.push token
    last_token = token
    i += 1
  return out_tokens

function code_in_strings (tokens, tokenizer)
  #allow double-quoted strings with embedded code, like: "x is #{x}"
  return tokens when tokenizer doesnt exist

  out_tokens = []
  for token in tokens
    if token.type is 'STRING' and token.value[0] is '"'
      rv = token.value
      r = /#{.*?}/g
      m = r.exec rv
      add_parens = yes if m otherwise no
      out_tokens.push({text:'(', line:token.line, value:'(', type:'LITERAL', soft:yes}) when add_parens
      while m
        new_token_text = rv.slice(0,m.index) + '"'
        out_tokens.push {text:new_token_text, line:token.line, value:new_token_text, type:'STRING'}
        out_tokens.push {text:'+', line:token.line, value:'+', type:'LITERAL'}
        new_tokens = tokenizer(rv.slice(m.index+2,m.index+m[0].length-1))[0]
        out_tokens.push({text:'(', line:token.line, value:'(', type:'LITERAL'}) when new_tokens.length isnt 1
        out_tokens = out_tokens.concat new_tokens
        out_tokens.push({text:')', line:token.line, value:')', type:'LITERAL'}) when new_tokens.length isnt 1
        rv = '"' + rv.slice(m.index+m[0].length)
        if rv is '""' #avoid adding a (+ "") to strings that end with #{expr}"
          rv = ''
        else
          out_tokens.push {text:'+', line:token.line, value:'+', type:'LITERAL'}
        r = /#{.*?}/g
        m = r.exec rv
      out_tokens.push({text:rv, line:token.line, value:rv, type:'STRING'}) when rv isnt ''
      out_tokens.push({text:')', line:token.line, value:')', type:'LITERAL', soft:yes}) when add_parens
    else
      out_tokens.push token
  return out_tokens

function multiline_lists (tokens)
  #allow lists to span multiple lines
  out_tokens = []
  list_depth = 0
  last_token_was_white = no
  indent_depths = []
  indent_depth = 0
  leftover_indent = 0
  for token in tokens
    skip_this_token = no
    token_is_white = (token.type in ['NEWLINE','INDENT', 'DEDENT'] or token.value is ',')
    if token.value is '[' or token.value is '{'
      list_depth += 1
      indent_depths.push indent_depth
      indent_depth = 0
    else if token.value is ']' or token.value is '}'
      list_depth -= 1
      leftover_indent = indent_depth
      indent_depth = indent_depths.pop()
    else if token.type is 'INDENT'
      indent_depth += 1
      if leftover_indent isnt 0
        leftover_indent += 1
        skip_this_token = yes
        out_tokens.push({text:'', line:token.line, value:'\n', type:'NEWLINE'}) if leftover_indent is 0
    else if token.type is 'DEDENT'
      indent_depth -= 1
      if leftover_indent isnt 0
        leftover_indent -= 1
        out_tokens.push({text:'', line:token.line, value:'\n', type:'NEWLINE'}) if leftover_indent is 0
        skip_this_token = yes
    else if token.type is 'NEWLINE'
      if leftover_indent isnt 0
        skip_this_token = yes
    else
      leftover_indent = 0

    if list_depth > 0
      if token_is_white and not last_token_was_white #the first token in a newline stretch gets turned into a comma
        out_tokens.push {text:',', line:token.line, value:',', type:'LITERAL'}
      else
        out_tokens.push token unless token_is_white or skip_this_token
    else
      out_tokens.push token unless skip_this_token
    last_token_was_white = token_is_white and (list_depth > 0)
  return out_tokens

function print_statement (tokens)
  new_tokens = []
  for token in tokens
    if token.value is 'print' and token.type is 'IDENTIFIER'
      new_tokens.push {text:'print', line:token.line, value:'console', type:'IDENTIFIER'}
      new_tokens.push {text:'print', line:token.line, value:'.', type:'LITERAL'}
      new_tokens.push {text:'print', line:token.line, value:'log', type:'IDENTIFIER'}
    else
      new_tokens.push token
  return new_tokens
